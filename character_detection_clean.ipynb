{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_letters_list2(im_name):\n",
    "    image = cv2.imread(im_name,0)\n",
    "    image = cv2.bitwise_not(image)\n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)\n",
    "    sizes = stats[1:, -1]\n",
    "    nb_components = nb_components - 1\n",
    "    min_size = 100\n",
    "    for i in range(0, nb_components):\n",
    "        if sizes[i] <= min_size:\n",
    "            image[output == i + 1] = 0\n",
    "    edge = 16\n",
    "    inv_cropped = image[edge:image.shape[0]-edge,edge:image.shape[1]-edge]\n",
    "    new_image, letters_only_contour, hierarchy = cv2.findContours(inv_cropped,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    letters_list = []\n",
    "    cropped = cv2.bitwise_not(inv_cropped)\n",
    "    # sorts contours by x position\n",
    "    starting_points = []\n",
    "    for cnt in letters_only_contour:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        starting_points.append(x)\n",
    "    sorted_letters_only_contour = [x for _,x in sorted(zip(starting_points,letters_only_contour))]\n",
    "    # append letters to list.  following code detects unually long letters, and assumes the contour found multiple letters\n",
    "    # splits such contours in half until it's a reasonable length\n",
    "    for cnt in sorted_letters_only_contour:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        temp_width = w\n",
    "        while temp_width > 34:\n",
    "            if temp_width < 60:\n",
    "                temp_width = np.floor_divide(temp_width,2)\n",
    "            elif temp_width < 91:\n",
    "                temp_width = np.floor_divide(temp_width,3)\n",
    "            else:\n",
    "                temp_width = np.floor_divide(temp_width,2)\n",
    "        for i in range(np.floor_divide(w,temp_width)):\n",
    "            letters_list.append(cropped[y:y+h,x+i*temp_width:x+(i+1)*temp_width])\n",
    "    while len(letters_list)<6:\n",
    "        letter_widths = []\n",
    "        for letter in letters_list:\n",
    "            letter_widths.append(letter.shape[1])\n",
    "        widest_letter_index = letter_widths.index(max(letter_widths))\n",
    "        widest_letter = letters_list[widest_letter_index]\n",
    "        letters_list.remove(widest_letter)\n",
    "        letters_list.insert(widest_letter_index,widest_letter[:,np.floor_divide(widest_letter.shape[1],2):])\n",
    "        letters_list.insert(widest_letter_index,widest_letter[:,0:np.floor_divide(widest_letter.shape[1],2)])\n",
    "    if len(letters_list)>6:\n",
    "        return ('An error has occurred.  Too many letters detected',letters_list)\n",
    "    else:\n",
    "        return letters_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_list = os.listdir('grainy_samples/_10samples')\n",
    "for image_name in image_list:\n",
    "    letters_list = get_letters_list2('grainy_samples/_10samples/%s'%image_name)\n",
    "    for letter in letters_list:\n",
    "        cv2.imshow('letters only',letter)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed of 16020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 images processed of 16020\n",
      "400 images processed of 16020\n",
      "600 images processed of 16020\n",
      "800 images processed of 16020\n",
      "1000 images processed of 16020\n",
      "1200 images processed of 16020\n",
      "1400 images processed of 16020\n",
      "1600 images processed of 16020\n",
      "1800 images processed of 16020\n",
      "2000 images processed of 16020\n",
      "2200 images processed of 16020\n",
      "2400 images processed of 16020\n",
      "2600 images processed of 16020\n",
      "2800 images processed of 16020\n",
      "3000 images processed of 16020\n",
      "3200 images processed of 16020\n",
      "3400 images processed of 16020\n",
      "3600 images processed of 16020\n",
      "3800 images processed of 16020\n",
      "4000 images processed of 16020\n",
      "4200 images processed of 16020\n",
      "4400 images processed of 16020\n",
      "4600 images processed of 16020\n",
      "4800 images processed of 16020\n",
      "5000 images processed of 16020\n",
      "5200 images processed of 16020\n",
      "5400 images processed of 16020\n",
      "5600 images processed of 16020\n",
      "5800 images processed of 16020\n",
      "6000 images processed of 16020\n",
      "6200 images processed of 16020\n",
      "6400 images processed of 16020\n",
      "6600 images processed of 16020\n",
      "6800 images processed of 16020\n",
      "7000 images processed of 16020\n",
      "7200 images processed of 16020\n",
      "7400 images processed of 16020\n",
      "7600 images processed of 16020\n",
      "7800 images processed of 16020\n",
      "8000 images processed of 16020\n",
      "8200 images processed of 16020\n",
      "8400 images processed of 16020\n",
      "8600 images processed of 16020\n",
      "8800 images processed of 16020\n",
      "9000 images processed of 16020\n",
      "9200 images processed of 16020\n",
      "9400 images processed of 16020\n",
      "9600 images processed of 16020\n",
      "9800 images processed of 16020\n",
      "10000 images processed of 16020\n",
      "10200 images processed of 16020\n",
      "10400 images processed of 16020\n",
      "10600 images processed of 16020\n",
      "10800 images processed of 16020\n",
      "11000 images processed of 16020\n",
      "11200 images processed of 16020\n",
      "11400 images processed of 16020\n",
      "11600 images processed of 16020\n",
      "11800 images processed of 16020\n",
      "12000 images processed of 16020\n",
      "12200 images processed of 16020\n",
      "12400 images processed of 16020\n",
      "12600 images processed of 16020\n",
      "12800 images processed of 16020\n",
      "13000 images processed of 16020\n",
      "13200 images processed of 16020\n",
      "13400 images processed of 16020\n",
      "13600 images processed of 16020\n",
      "13800 images processed of 16020\n",
      "14000 images processed of 16020\n",
      "14200 images processed of 16020\n",
      "14400 images processed of 16020\n",
      "14600 images processed of 16020\n",
      "14800 images processed of 16020\n",
      "15000 images processed of 16020\n",
      "15200 images processed of 16020\n",
      "15400 images processed of 16020\n",
      "15600 images processed of 16020\n",
      "15800 images processed of 16020\n",
      "16000 images processed of 16020\n",
      "198 files could not not be parsed (errors)\n"
     ]
    }
   ],
   "source": [
    "image_list = os.listdir('grainy_samples/samples')\n",
    "num_images = len(image_list)\n",
    "count = 0\n",
    "errors = 0\n",
    "for image_name in image_list:\n",
    "    letters_list = get_letters_list2('grainy_samples/samples/%s'%image_name)\n",
    "    if len(letters_list)==6:\n",
    "        for i in range(len(letters_list)):\n",
    "            letter_key = image_name[i]\n",
    "            cv2.imwrite('grainy_samples/letters/%s/%s.png'%(letter_key,count),letters_list[i])\n",
    "            count+=1\n",
    "    else:\n",
    "        errors+=1\n",
    "    image_index = image_list.index(image_name)\n",
    "    if image_index % 200 ==0:\n",
    "        print('%d images processed of %d'%(image_index,num_images))\n",
    "print('%d files could not not be parsed (errors)'%errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaavcc.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
